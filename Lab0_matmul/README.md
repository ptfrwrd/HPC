# Умножение матриц различных размеров на CPU/GPU

- Jupyter notebook был написан с использованием google colab pro;
- Для работы с GPU была использована библиотека pycuda;
- Были произведены замеры времени работы на CPU/GPU и посчитано ускорение, результаты приведены ниже (округление до 4 цифры после запятой):

<table>
<thead>
<tr><th>Size</th><th>CPU time</th><th>GPU time</th><th>Acceleration</th></tr>
</thead>
<tbody>
<tr><td>128</td><td> 1.5637</td><td>0.001</td><td>1609.0915</td></tr>
<tr><td>256</td><td> 11.5251</td><td>0.0032</td><td>3599.9234</td></tr>
<tr><td>512</td><td> 93.4048</td><td>0.0113</td><td>8254.5239</td></tr>
<tr><td>1024</td><td> 859.1499</td><td>0.0493</td><td>17435.5929</td></tr>
<tr><td>2048</td><td> 6723.3339</td><td>0.326</td><td>20621.3875</td></tr>
</tbody>
</table>

- Программа перемножает квадратные матрицы размером Size x Size;
- Была использована собственная реализация как на GPU, так и на CPU;
- Для GPU: создается блок потоков (block_size), на его основе определяется grid_size;
- GPU-функция перемножения матриц представляет собой ядро (на C), которое преобразуется в Python-код;
- Также результаты (время/размер матрицы) для CPU/GPU были визуализированы.